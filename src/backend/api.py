import logging
import pdb

from fastapi import Request, APIRouter
from pydantic import BaseModel

# Set up logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)

router = APIRouter()

class ChatRequest(BaseModel):
    prompt: str
    system_message: str
    

@router.get("/")
def read_root():
    return {"message": "Chat API is running. Use POST /chat endpoint."}

@router.post("/chat")
def chat(req: ChatRequest, request: Request):
    """Handle chat requests by generating a response from the LLaMA model.
    Note that `request` is generated by FastAPI for internal representation for backend. (Not sent by the client)
    """
    llm = request.app.state.llm
    logger.debug(f"Using LLM of type: {type(llm)}")

    system_message = req.system_message
    logger.debug(f"Using system message: {system_message[:50]}...")

    # For Llama 3.2 - using chat format
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": req.prompt}
        ]

    #TODO: examine max_tokens and stop parameters
    output = llm.create_chat_completion(
        messages=messages, max_tokens=1000, temperature=1, repeat_penalty=1.2
        )
    logger.debug(f"Finish reason: {output["choices"][0]["finish_reason"]}")
    return {"response": output["choices"][0]["message"]["content"].strip()}